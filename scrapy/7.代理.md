+ 下载中间件
    - scrapy中设置代理 
        1. 内置
            ```python
            # 在爬虫启动时，提前在os.envrion中设置代理即可。
            # 方式一
            class ChoutiSpider(scrapy.Spider):
                name = 'chouti'
                allowed_domains = ['chouti.com']
                start_urls = ['https://dig.chouti.com/']
                cookie_dict = {}

                def start_requests(self):
                    import os
                    os.environ['HTTPS_PROXY'] = "http://root:woshiniba@192.168.11.11:9999/"
                    os.environ['HTTP_PROXY'] = '19.11.2.32',
                    for url in self.start_urls:
                        yield Request(url=url,callback=self.parse)
            # 方式二  meta
            class ChoutiSpider(scrapy.Spider):
                name = 'chouti'
                allowed_domains = ['chouti.com']
                start_urls = ['https://dig.chouti.com/']
                cookie_dict = {}

                def start_requests(self):
                    for url in self.start_urls:
                        yield Request(url=url,callback=self.parse,meta={'proxy':'"http://root:woshiniba@192.168.11.11:9999/"'})```
            # 同时设置则先看meta，没有meta则看环境变量里设置的代理

        2. 自定义
            ```python
            # by luffycity.com
            import base64
            import random
            from six.moves.urllib.parse import unquote
            try:
                from urllib2 import _parse_proxy
            except ImportError:
                from urllib.request import _parse_proxy
            from six.moves.urllib.parse import urlunparse
            from scrapy.utils.python import to_bytes

            class XdbProxyMiddleware(object):

                def _basic_auth_header(self, username, password):
                    user_pass = to_bytes(
                        '%s:%s' % (unquote(username), unquote(password)),
                        encoding='latin-1')
                    return base64.b64encode(user_pass).strip()

                def process_request(self, request, spider):
                    PROXIES = [
                        "http://root:woshiniba@192.168.11.11:9999/",
                        "http://root:woshiniba@192.168.11.12:9999/",
                        "http://root:woshiniba@192.168.11.13:9999/",
                        "http://root:woshiniba@192.168.11.14:9999/",
                        "http://root:woshiniba@192.168.11.15:9999/",
                        "http://root:woshiniba@192.168.11.16:9999/",
                    ]
                    url = random.choice(PROXIES)

                    orig_type = ""
                    proxy_type, user, password, hostport = _parse_proxy(url)
                    proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))

                    if user:
                        creds = self._basic_auth_header(user, password)
                    else:
                        creds = None
                    request.meta['proxy'] = proxy_url
                    if creds:
                        request.headers['Proxy-Authorization'] = b'Basic ' + creds
            ```

+ 配置（`settings.py`）
    ```python
    DOWNLOADER_MIDDLEWARES = {
    #    'xdb.middlewares.XdbDownloaderMiddleware': 543,
        xdb.proxy.XdbProxyMiddleware: 751,
    }
    ```

+ 查看默认设置的优先级   
    `scrapy/settings/default_settings.py`