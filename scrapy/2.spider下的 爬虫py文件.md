+ 代码解析
```python
# -*- coding: utf-8 -*-
import scrapy

# import sys,os,io
# sys.stdout=io.TextIOWrapper(sys.stdout.buffer,encoding='gb18030')

class ChoutiSpider(scrapy.Spider):
    name = 'chouti' # 不可以删除，代表爬虫的名称
    allowed_domains = ['chouti.com'] # 允许的域名，若<a>标签的链接不是该域名则不爬取该页面
    start_urls = ['http://chouti.com/'] # 开始爬虫的页面，之后回调parse函数

    def parse(self, response): # response的方法可以通过 "from scrapy.http.response.html import HtmlResponse"的"HtmlResponse"进行查看，即response是HtmlResponse的对象，包含相应的所有信息
        print(response.text) # text在源码中是@property，所以不用加括号

        # 去子孙中找div并且id=content-list
        # 选择器的内容另外讲解
        f = open('news.log', mode='a+')
        item_list = response.xpath('//div[@id="content-list"]/div[@class="item"]')
        for item in item_list:
            text = item.xpath('.//a/text()').extract_first()
            href = item.xpath('.//a/@href').extract_first()
            print(href,text.strip())
            f.write(href+'\n')
        f.close()

        page_list = response.xpath('//div[@id="dig_lcpage"]//a/@href').extract()
        for page in page_list:
            from scrapy.http import Request
            page = "https://dig.chouti.com" + page
            yield Request(url=page,callback=self.parse) # 继续爬取网页中的网页内容

```
+ `name`在创建爬虫时就已经指定
+ `allowed_domains`指定一个列表，列表内包含可以爬取的域名后缀
+ `start_urls`指定爬虫开始的起始url列表，程序开始会对其进行爬虫，响应结果默认会交给`parse`函数进行解析
+ `parse`函数名称暂不得更改，形参`response`是爬虫的响应结果，可通过`from scrapy.http.response.html import HtmlResponse`对其方法和属性进行查看
+ 对网页内的其他链接进行爬取，需要通过以下代码引入Request类
    ```python
    from scrapy.http import Request

    yield Request(url=page,callback=self.parse)
    # url指定需要进行爬虫的网址，callback则是指定爬虫完成后的相应内容要交给哪个解析函数
    ```