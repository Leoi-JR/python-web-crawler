+ 默认的去重规则
    ```python
    from scrapy.dupefilter import RFPDupeFilter # RFPDupeFilter是scrapy自带的去重规则
    ```

+ 自定义去重规则
    1. 编写类 (`dupefilters.py`)
        ```python
        from scrapy.dupefilter import BaseDupeFilter
        # www.baidu.com?k1=1&k2=2和www.baidu.com?k2=2&k1=1实际是同一个网址，用md5会生成两个不一样的结果，不适合，而scrapy已经写好，使用下面的方法即可
        from scrapy.utils.request import request_fingerprint

        class XdbDupeFilter(BaseDupeFilter):

            def __init__(self): # 第二步
                self.visited_fd = set()

            @classmethod 
            def from_settings(cls, settings): # 第一步
                return cls()

            def request_seen(self, request): # 第四步
                fd = request_fingerprint(request=request) # 需要传入Requset对象
                if fd in self.visited_fd:
                    return True
                self.visited_fd.add(fd)

            def open(self):  # 第三步 can return deferred
                print('开始')

            def close(self, reason):  # 第五步 can return a deferred
                print('结束')

            # def log(self, request, spider):  # log that a request has been filtered
            #     print('日志')
        ```

	2. 配置 (`settings.py`)
        ```python
        # 修改默认的去重规则
        # DUPEFILTER_CLASS = 'scrapy.dupefilter.RFPDupeFilter' # 默认使用scrapy自带的去重规则
        DUPEFILTER_CLASS = 'xdb.dupefilters.XdbDupeFilter' # 自定义的去重规则路径
        ```

	3. 爬虫使用：
        ```python
        class ChoutiSpider(scrapy.Spider):
            name = 'chouti'
            allowed_domains = ['chouti.com']
            start_urls = ['https://dig.chouti.com/']

            def parse(self, response):
                print(response.request.url)

                page_list = response.xpath('//div[@id="dig_lcpage"]//a/@href').extract()
                for page in page_list:
                    from scrapy.http import Request
                    page = "https://dig.chouti.com" + page
                    # dont_filter默认为False,表示遵循去重规则；True则不遵循去重规则
                    # yield Request(url=page,callback=self.parse,dont_filter=False) # 会根据配置的去重规则去重
                    yield Request(url=page,callback=self.parse,dont_filter=True) # 不去重
        ```
		
