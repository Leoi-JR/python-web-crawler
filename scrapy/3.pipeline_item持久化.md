+ pipeline/items
    1. 先写pipeline类
        ```python
        class XXXPipeline(object):
            def process_item(self, item, spider):
                return item
        ```
                    
    2. 写Item类
        ```python
        class XdbItem(scrapy.Item):
            href = scrapy.Field()
            title = scrapy.Field()
        ```
                    
    3. 配置
        ```python
        ITEM_PIPELINES = {
            'xdb.pipelines.XdbPipeline': 300,
        }
        ```
    
    4. 爬虫，yield每执行一次，process_item就调用一次。
        在`parse`函数中`yield Item对象`
        ```python
        yield Item(href = text1, title = text2)
        ```

+ 编写更全的pipeline：
    - 源码内容，代码执行顺序：
        1. 判断当前`XdbPipeline`类中是否有`from_crawler`  
            有：  
                内部会通过类直接调用函数  
                `obj = XdbPipeline.from_crawler(....)`  
            否：
                创建实例对象  
                `obj = XdbPipeline()`
        2. `obj.open_spider()`
        
        3. `obj.process_item()/obj.process_item()/obj.process_item()/obj.process_item()/obj.process_item()`执行多次
        
        4. `obj.close_spider()`
    -   
        ```python
        from scrapy.exceptions import DropItem

        class FilePipeline(object):

            def __init__(self,path): # 规范写法
                self.f = None
                # 如果没有from_crawl函数则形参只要有self即可
                self.path = path

            @classmethod # 有from_crawler函数会先执行此函数
            def from_crawler(cls, crawler):
                # 此函数目的是用于获取配置文件的信息，如获取数据库地址、存储文件地址等
                """
                初始化时候，用于创建pipeline对象
                :param crawler:
                :return:
                """
                print('File.from_crawler')
                # 利用crawler.settings.get()获取最全配置，通过import settings只能获取用户自定义的设置
                path = crawler.settings.get('HREF_FILE_PATH')
                # 有返回值，所以init构造函数要设置形参进行接收利用
                return cls(path)

            def open_spider(self,spider):
                """
                爬虫开始执行时，调用
                :param spider:
                :return:
                """
                # pipeline是所有爬虫公用的，所以如果只希望某一爬虫使用，要添加如下判断，判断其爬虫名称是否是所期待的
                # if spider.name == 'chouti':
                print('File.open_spider')
                # 打开文件
                self.f = open(self.path,'a+')

            def process_item(self, item, spider):
                print('File',item['href'])
                # 存储数据
                self.f.write(item['href']+'\n')
                
                # return item  	# 交给下一个pipeline的process_item方法
                raise DropItem()# 用于表明优先级在此pipeline之后的pipeline的process_item方法不再执行

            def close_spider(self,spider):
                """
                爬虫关闭时，被调用
                :param spider:
                :return:
                """
                print('File.close_spider')
                # 关闭文件
                self.f.close()
        ```

